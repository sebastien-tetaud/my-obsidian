[[Deep Learning]] [[Data Visualization]]
## Introduction

Sentinel-2 satellite imagery is distributed in different processing levels, with Level-1C (L1C) and Level-2A (L2A) being the most widely used. The transformation from L1C to L2A represents a critical step in making satellite imagery ready for scientific analysis and practical applications.

## Understanding the Processing Levels

**Level-1C (L1C)** products provide top-of-atmosphere (TOA) reflectance data. While geometrically corrected and accurately georeferenced, L1C data still contains atmospheric effects—such as scattering and absorption by gases, aerosols, and clouds—that can significantly obscure the Earth's surface features.

**The Sentinel-2 Level-2A**  product provides orthorectified Surface Reflectance (Bottom-Of-Atmosphere: BOA), with sub-pixel multispectral and multitemporal registration accuracy. Scene Classification (including Clouds and Cloud Shadows), AOT (Aerosol Optical Thickness) and WV (Water Vapour) maps are included in the product. L2A is generated by Sen2core Algorithms.
https://step.esa.int/thirdparties/sen2cor/2.11.0/docs/Sen2Cor_Quick_Guide_V_1_2_2.pdf


Sen2Cor is a processor for Sentinel-2 Level 2A product generation and formatting; it performs the atmospheric-, terrain- and cirrus correction of mono-temporal Top-Of-Atmosphere Level 1C input data. Sen2Cor creates Surface Reflectance (optionally terrain- and cirrus- corrected) images and, additionally, Aerosol Optical Thickness Map, Water Vapour Map, Scene Classification Map and Quality Indicators for cloud and snow probabilities. Its output product format is inherited from the Level 1C User Product: JPEG 2000 images, with three different resolutions, 60, 20 and 10 m.

### Processing time of Sen2Cor

https://forum.step.esa.int/t/reducing-sen2cor-processing-time/1675/4
## Challenges and Future Directions

Despite the potential, several challenges remain. Training data acquisition is difficult, as "ground truth" BOA reflectance typically relies on already-processed L2A products or ground measurements. Additionally, generalizing across diverse atmospheric conditions and landscapes remains challenging.

## Dataset Generation 


To the start the project with a very small dataset, we choose a small area on the Earth where we will get the product information using STAC catalogue. 
The generation of the dataset is crucial to understand if a Deep Learning approach allow to create a model that will take as input L1C product and map into :2A product. 
The data available for Sentinel-2  the CDSE are the following: 

Level 1C:

- Band 1 (60m)
- Band 2 (10m)
- Band 3 (10m)
- Band 4 (10m)
- Band 5 (20m)
- Band 6 (20m)
- Band 7 (20m) 
- Band 8 (10m)
- Band 9 (60m)
- Band 10 (60m)
- Band 11 (20m)
- Band 12 (20m)
- Band 9 (60m)
- Band 8A (20m)
- TCI (10m)-> (Band 2, Band 3, Band 4) (10m)

Level 2C:

- Band 1 (60m)
- Band 2 (10m)  (20m)  (60m)
- Band 3 (10m)  (20m)  (60m)
- Band 4 (10m)  (20m)  (60m)
- Band 5 (20m)
- Band 6 (20m)
- Band 7 (20m) 
- Band 8 (10m)
- Band 9 (60m)
- Band 10 (60m)
- Band 11 (20m)
- Band 12 (20m)
- Band 9 (60m)
- Band 8A (20m)
- TCI (10m)-> (Band 4, Band 3, Band 2) (10m)  (20m)  (60m)

The TCI color is generated with generate with the Band 2, Band 3 and Band 2
https://custom-scripts.sentinel-hub.com/sentinel-2/l2a_optimized/

# Sentinel-2 Dataset Preparation: Central Europe Study Area

## Data Selection and Preprocessing

For the initial version of our dataset, we strategically selected a study area in Central Europe that have diverse landscapes, including urban centers, agricultural regions, forests, and water bodies. This geographical diversity ensures our model encounters a wide range of surface features and atmospheric conditions during training.

## Imagery Resolution and Processing

We chose to work with Sentinel-2's True Color Imagery (TCI) composites, which provide natural-color representations of the landscape by combining the red, green, and blue spectral bands. While Sentinel-2 offers TCI at 10m native resolution, our experimental approach required data at 60m resolution for several reasons:

1. **Computational efficiency** during the development and initial training phases
2. **Reduced storage requirements** while maintaining sufficient spatial detail
3. **Experimental validation** before scaling to higher resolutions

## Downscaling Methodology

Since the Copernicus Data Space Ecosystem (CDSE) does not directly provide TCI products at 60m resolution, we implemented a systematic downscaling workflow:

1. We acquired both L1C (top-of-atmosphere) and L2A (bottom-of-atmosphere) TCI products at their native 10m resolution
2. We applied a bicubic interpolation algorithm for downscaling, which preserves edge and gradient information better than simpler resampling methods
3. This process was applied consistently to both L1C and L2A imagery to maintain paired relationships for the deep learning model



```python
min_lon, min_lat, max_lon, max_lat = 149,-20, 151, -22
bbox_polygon = {

"type": "Polygon",
"coordinates": [[
[min_lon, min_lat], # Southwest corner
[max_lon, min_lat], # Southeast corner
[max_lon, max_lat], # Northeast corner
[min_lon, max_lat], # Northwest corner
[min_lon, min_lat]  # Close the polygon by repeating the first point
]]
}
```


Query parameters:
Bounding box: [3.2833, 45.3833, 11.2, 50.1833]
Date range: 2020-01-01 to 2025-01-01
Max items per request: 1000
Max cloud cover: 100%

PNSR = 21.7 for each bands in average

Example of url to test the code
band_s3_url = "/Sentinel-2/MSI/L1C/2025/01/01/S2A_MSIL1C_20250101T104441_N0511_R008_T32UMA_20250101T142131.SAFE/GRANULE/L1C_T32UMA_A049769_20250101T104604/IMG_DATA/T32UMA_20250101T104441_TCI.jp2"



# Computing Loss Only for Valid Pixels in Satellite Images

When dealing with satellite imagery that contains no-data regions (like acquisition borders), we need to mask out these invalid pixels when computing your loss. Here are several approaches you can implement:
## 1. Using a Mask for Valid Pixels

```python
for batch_idx, (x_data, y_data) in enumerate(train_loader):

	x_data = x_data.to(device)
	y_data = y_data.to(device)
	valid_mask = (y_data >= 0)
	outputs = model(x_data)
	loss = criterion(outputs[valid_mask], y_data[valid_mask])
	loss.backward()
	optimizer.step()
```

## Data Driven metrics and Physical Metrics


- Threshold Accuracy metric is used in Depth Estimation to evaluate the performance of the model. The Threshold accuracy measures the percentage of predicted pixels that differ from the true pixels by no more than 25 %.


### Peak Signal-to-Noise Ratio (PSNR)


- PSNR: The Peak Signal-to-Noise Ratio (PSNR) is a widely used metric to evaluate the quality of reconstructed images. When dealing with multi-spectral or hyper-spectral data, such as those acquired by satellite sensors like Sentinel-2, it is crucial to calculate PSNR for each band separately. This is because different bands capture different aspects of the Earth's surface, such as vegetation health, water quality, or soil composition, and have varying levels of signal quality. By computing PSNR for each band, we can assess the accuracy of the  prediction model for each specific spectral channel, allowing for a more nuanced understanding of its performance. By examining the PSNR values for each band, you can gain a deeper understanding of which bands are most accurately reconstructed and which may be more challenging for the model, ultimately informing strategies for model improvement and optimization.

- **Purpose**: Measures overall image quality and error magnitude
- **Strength**: Standard metric in image processing, easy to interpret
- **Calculation**: Based on MSE but scaled logarithmically (higher is better)
- **Limitation**: May not align with perceptual quality or spectral accuracy

### Root Mean Square Error (RMSE)

- **Purpose**: Quantifies absolute prediction error in physical units
- **Strength**: Direct measure of radiometric accuracy
- **Calculation**: Square root of average squared difference between prediction and target
- **Limitation**: Sensitive to outliers, doesn't capture structural information

### Structural Similarity Index (SSIM)

- **Purpose**: Evaluates preservation of structural information
- **Strength**: Better aligned with human visual perception (higher is better)
- **Calculation**: Compares luminance, contrast and structure between images


### Spectral Angular Mapper (SAM)

- **Purpose**: Measures spectral similarity independent of brightness
- **Strength**: Directly evaluates atmospheric correction quality for spectral applications
- **Calculation**: Angle between pixel vectors in multi-dimensional spectral space
- **Limitation**: Doesn't capture spatial or structural information

### Some results

The V1 dataset has demonstrated the feasibility of generating L2A products from L1C products. However, in this dataset version, cloud coverage information is missing. The lack of cloud coverage data can be a problem for understanding the data distribution and may potentially impact the learning process. A significant limitation of this dataset is that it does not allow for future improvements, such as analyzing the average cloud ratio within the training, validation, and testing data, due to the absence of this information. 
Analysis of the training and validation loss reveals slight overfitting, which can be attributed to the similarity of the areas of interest in the dataset.

Moreover, the TCI product is product made of 3 bands with color adjustment technics for visualization purposes. Since we know it is possible to train a model, now we can take the native band at 10m + cloud coverage. The Version 2 of dataset will still cover the same area and some data cleaning will be done to remove the data that contain only 0 and 1.


## Dataset V2

- Same area as V1 area.
- 10m bands downscale to 60m.
- Cloud coverage.
- Split the dataset based on cloud coverage distribution.
- Remove useless Tile.

## Sparse distribution of Valid Pixel 

To address the impact of varying valid pixel percentages in your batch (with some samples having 80% and others only 20% valid pixels), I recommend implementing a sample-weighted loss function. This approach calculates individual losses for each sample, then weights them based on their valid pixel percentage before averaging. This prevents sparse samples from being underrepresented while mitigating the risk of outliers in those samples disproportionately affecting training. You can choose between proportional weighting (favoring dense samples), equal weighting (treating all samples equally regardless of valid pixel count), or square root scaling (a balanced compromise). Additionally, you can incorporate outlier detection specifically for sparse samples to further reduce their negative impact on model learning.

# Weighted Loss for Images with Varying Valid Pixel Percentages: A Summary

## Problem Statement
When training models on datasets where images have varying numbers of valid pixels (due to masks, missing data, etc.), standard loss functions face challenges:
- Images with few valid pixels contribute less to the overall loss
- Models may ignore or underperform on samples with limited valid regions
- Training becomes biased toward samples with more valid pixels

## Proposed Solution: Valid Pixel Weighted Loss

### Key Components:
1. **Valid Pixel Detection**: Create binary masks identifying valid pixels (where ground truth ≥ 0)
2. **Per-Sample Weighting**: Weight each sample based on its valid pixel ratio
   - Calculate ratio: r = valid_pixels / total_pixels
   - Apply square root transformation: weight = √r 
   - This balances between equal sample weighting and equal pixel weighting
3. **Loss Computation**: Calculate loss only on valid pixels for each sample
4. **Weighted Aggregation**: Sum the weighted sample losses
5. **Normalization**: Divide by the total weights to get a properly scaled average

### Algorithm Implementation
```python
def valid_pixel_weighted_loss(y_pred, y_true, base_loss_fn):
    batch_size = y_pred.shape[0]
    total_weighted_loss = 0.0
    total_weight = 0.0
    
    for i in range(batch_size):
        # Get masks for valid pixels (where ground truth >= 0)
        valid_mask = (y_true[i] >= 0)
        valid_pixel_count = valid_mask.sum()
        total_pixel_count = valid_mask.size
        
        # Skip samples with no valid pixels
        if valid_pixel_count == 0:
            continue
            
        # Calculate weight using square root of valid pixel ratio
        valid_ratio = valid_pixel_count / total_pixel_count
        weight = math.sqrt(valid_ratio)
        
        # Compute loss only on valid pixels
        sample_loss = base_loss_fn(
            y_pred[i][valid_mask], 
            y_true[i][valid_mask]
        )
        
        # Add weighted loss to total
        total_weighted_loss += sample_loss * weight
        total_weight += weight
    
    # Return normalized weighted average
    if total_weight > 0:
        return total_weighted_loss / total_weight
    return 0.0  # Handle edge case with no valid pixels
```

## Importance of Normalization
Dividing by the total weight (`total_weighted_loss / total_weight`) is crucial because it:

1. **Provides Scale Invariance**: Loss values remain comparable regardless of batch composition
2. **Stabilizes Gradients**: Prevents gradient explosion/vanishing due to varying batch validity
3. **Ensures Balanced Learning**: Samples contribute proportionally to their importance, not simply to batch size
4. **Maintains Statistical Interpretation**: Result represents expected loss per weighted sample

Without normalization, the loss scale would fluctuate with batch composition, making training unstable and hyperparameter tuning difficult.

## Benefits of This Approach
- **Fairness**: Even samples with few valid pixels receive appropriate attention
- **Stability**: Training progresses consistently regardless of valid pixel distribution
- **Flexibility**: Works with any base loss function (MSE, cross-entropy, etc.)
- **Balance**: Square root weighting prevents both extremes - treating all samples equally or all pixels equally

This weighted loss approach effectively handles the complexities of training on datasets with variable valid pixel counts while maintaining training stability.